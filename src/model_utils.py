import torch
import torch.nn as nn
import torch.nn.functional as F

class GlobalMaxPooling(nn.Module):
    def __init__(self, dim=-1):
        super(self.__class__, self).__init__()
        self.dim = dim
        
    def forward(self, x):
        return x.max(dim=self.dim)[0]

class CommentsEncoder(nn.Module):
    def __init__(self, n_tokens, PAD_IX, out_size=64):
        """ 
        A simple sequential encoder for titles.
        x -> emb -> conv -> global_max -> relu -> dense
        """
        super(self.__class__, self).__init__()
        self.emb   = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)
        self.conv1 = nn.Conv1d(64, out_size, kernel_size=3, padding=1)
        self.pool1 = GlobalMaxPooling()        
        self.dense = nn.Linear(out_size, 6)
    
    def forward(self, text_ix):
        """
        :param text_ix: int64 Variable of shape [batch_size, max_len]
        :returns: float32 Variable of shape [batch_size, out_size]
        """
        h = self.emb(text_ix)

        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order
        h = torch.transpose(h, 1, 2)
        
        # Apply the layers as defined above. Add some ReLUs before dense.
        h = self.conv1(h)
        h = self.pool1(h)
        h = F.relu(h)
        h = self.dense(h)
        
        return h

class CommentsLSTMEncoder(nn.Module):
    def __init__(self, n_tokens, PAD_IX, out_size=64):
        """ 
        A simple sequential encoder for titles.
        x -> emb -> conv -> global_max -> relu -> dense
        """
        super(self.__class__, self).__init__()
        self.emb   = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)
        self.lstm1 = nn.LSTM(64, out_size, num_layers=1)
        self.pool1 = GlobalMaxPooling()        
        self.dense = nn.Linear(out_size, 6)
    
    def forward(self, text_ix):
        """
        :param text_ix: int64 Variable of shape [batch_size, max_len]
        :returns: float32 Variable of shape [batch_size, out_size]
        """
        h = self.emb(text_ix)

        # Apply the layers as defined above. Add some ReLUs before dense.
        output, (h_n, c_n) = self.lstm1(h)
        h = self.pool1(output)
        h = F.relu(h)
        h = self.dense(h)
        
        return h



def compute_loss(prediction, reference):
    """
    Computes objective for minimization.
    """
    loss = nn.CrossEntropyLoss()
    return loss(prediction, reference).cuda()

def compute_accuracy(reference, prediction):
    _, predicted = torch.max(prediction, 1)
    return (predicted == reference).squeeze().sum().item()